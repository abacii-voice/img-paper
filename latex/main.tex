%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,11pt,5p,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\pagenumbering{gobble}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Journal Name}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Image Paper}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Nicholas Piano}

\address{Cambridge University}

\begin{abstract}

%% Text of abstract
Accurate cell segmentation is crucial to biological and medical studies in providing representative data of cell movements and shapes to judge the effects of drugs and other environmental changes. Current methods rely on outlining the cell projected in 2D or a volumetric approximation based on 3D data. A common imaging method is confocal microscopy, where the images stored capture a range of focal planes in an environment. This provides 3D data and the ability to move an object of interest in and out of focus after the images have been captured, potentially offering accurate segmentation. However, conventional segmentation software fails to fully exploit the 3D nature of the data and is unable to make complex associations between image channels.

Here we describe a method for selecting regions of the brightfield channel that contain clear edges suitable for segmentation by determining the XYZ coordinates of bright regions in the GFP. This exploits the relationship between the coordinate systems of the image sets, since the majority of the 3D set of brightfield images is blurred and unsuitable for segmentation. Images of the environment can thus be simplified into a 2D projection that can be readily processed by CellProfiler or ImageJ. This method has been tested on breast cancer cells in a PDMS micro-fluidics environment and yields clear segmentation. This is an improvement over previous 2D methods of image pre-processing, such as maximum or mean Z-projection, which are unable to extract usable information from the 3D environment.

\end{abstract}

\begin{keyword}
Science \sep Publication \sep Complicated
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text
\section{Introduction}
\label{S:1}

\begin{figure}
\centering\includegraphics[width=1.0\linewidth]{../img/fig1/v2/fig1_v2_annotated_names}
\label{fig:method_flowchart}
\caption{A graphical layout of the method used. At the top, the process starts with the original data for each frame; the brightfield image stack and the corresponding GFP channel. The [[[zMod]]] image and the [[[zVar]]] images are generated from the GFP alone by observing the GFP profile in Z. As outlined in Section~\ref{S:2.gfpprofile}, the [[[zMod]]] image gives the Z location of the maximum value in the GFP profile for each pixel. High value pixels represent higher Z values. The [[[zVar]]] image is generated by extracting the variance of each profile and assigning that value to each pixel. Both of these images can be modified using the [[[R]]] and [[[Sigma]]] parameters. The [[[zBF]]] image is the most important result of this study, combining the 3D information found using the GFP in [[[zMod]]] and the full range of the brightfield image stack, it places pixel values from Z levels where objects are in focus, allowing them to be accurately segmented. This data can be taken further using the [[[zUnique]]] and [[[zEdge]]] images. The [[[zUnique]]] image uses the maximum value for each unique value of Z in [[[zMod]]] and substitutes this value for the whole image. This yields high values within all parts of cells found at the same level, regardless of intensity or profile strength. The [[[zEdge]]] image superimposes the edge of [[[zUnique]]] for more confined segmentation.}
\end{figure}

\begin{figure*}
\centering\includegraphics[width=1.0\linewidth]{../img/fig2/v1/fig2_v1_annotated}
\label{fig:zbf}
\caption{This is a side-by-side comparison of the zBF modi cation with a single slice of the original bright eld data. The changes that have been made are mostly the adjustment of focus in the appearance of cells that are marked with GFP. Note  rst of all that cells a) and b) have not been adjusted. These are not marked with GFP and are not affected by any of the algorithms. Cells c), d), and e) originally appear to be out of focus in image A, but in B, they appear in focus. This is part of the correction made. The cells in group f) have been adjusted, but their corrections lie much closer to the focus they originally occupied in this slice. They still appear in focus. All cells marked with GFP have been level corrected. This is the most powerful result of this study. C and E show the modified brightfield and the maximum projection of the GFP respectively, D and F show the possible segmentation of the data. Although better segmentation can be achieved for the GFP, this segmentation was not tailored to this cell and parameters were adjusted to match the environment as a whole. The corrected brightfield on the other hand is much easier to find consistent parameters for.}
\end{figure*}

\begin{figure}
\centering\includegraphics[width=1.0\linewidth]{../img/fig3/v3/fig3_v3_annotated}
\label{fig:segmentation}
\caption{A shows the corrected brightfield. B is an example of the segmentation that is possible after the images have been corrected. Rather than trying to segment blurred images or incomplete GFP images, the data can be simplified into a useable format. Marker numbers indicate the order in which new objects were recognised and only part of the environment is shown. CDEF show an area comparison between the corrected [[[zEdge]]] (red), the maximum GFP projection (green), an intermediate stage of the process for comparison (cyan), and the Seminummi brightfield projection method (blue). Each plot shows the values for a single cell over its time series. The values are calculated using the following formula: $n_t = 1 - \frac{d_{max} - d_t}{d_{max}}$, where $d_t = \abs{G_t - A_t}$ and $d_{max} = \abs{G_t - A_{max}}$, $G$ is the Ground Truth or manually segmented area, $A$ is the segmentated area, and $t$ is the frame index. $n$ is a measurement of how close the segmented value is to the Ground Truth relative to the other segmentation results. A high value is favoured since it is inverted.}
\end{figure}

Accurate measurement of cell morphodynamics is an essential part of live-cell studies. Cell shape can be measured by distinguishing dark edges and bright cell interiors and differentiating them from the background of the image in a process called segmentation. Newly developed microfluidics techniques allow in vitro studies of complex 3D environments. In order to study live cells moving in this environment, 3D imaging is done using a confocal microscope. Cell segmentation is then done using image recognition software such as CellProfiler or ImageJ. Ideally, the cell shape should be captured and analysed in 3D, but imaging limitations can prevent an adequate 3D reconstruction. These limitations include cell overheating from excessive contact with the laser used to produce a fluorescent response, lower resolution from a larger field of view that allows more cells to be imaged, and the necessary time delay between frames to allow the entire field to be imaged. These factors contribute to lower image quality, which prevents the extraction of high-quality shape data from the images. A possible solution is to use the relatively high-quality brightfield images to determine the shape of the cell.

In their 2009 paper, Selinummi et al. attempt to solve this problem by scanning the brightfield in 3D. Although brightfield data does not contain 3D information, intensity at a location will vary as an object moves into and out of focus by moving the current Z plane. This variation can indicate the presence of an object, and can be quantified mathematically to produce a highlighted image showing areas likely to contain objects. This works to a degree in a simple environment, but fails in more complex scenarios. It is unable to determine the Z location of individual objects, differentiate between objects such as different cell types and PMDS (a polymer used as a structural material), or accurately represent the boundary of a cell due to distortion.

In this study, we used 3D information from a Green Fluorescent Protein (GFP) channel to allow the segmentation of brightfield images by finding levels with clear edges. This exploits the relationship between the coordinate systems of the image sets. Conventional cell segmentation software such as CellProfiler fails when presented with a 3D set of brightfield data. The method presented in this study pre-processes the 3D image data to allow segmentation via conventional means. Since the coordinate systems of the brightfield and GFP images represent the same space, high intensity GFP at a single level, Z, and at a fixed location, XY, can indicate the presence of clear, dark edges of an object in the brightfield. For every point, XY, in the image, the corresponding Z level of high intensity GFP can be determined. This can then be used to combine all the high-quality edges into a single 2D image, which is then ready for conventional 2D segmentation.

\section{Materials and methods}
\label{S:method}

\subsection{Microscope apparatus}
Give all details of the microscope setup including makes of equipment.

1. Make of microscope, serial number
2. Pinhole size, aperture, other camera properties
3. Type of cancer cells, size, speed, etc.
4. Contrast, histograms, laser intensity
5. GFP type, staining method, timescales.
6. Timing of job imaging

\subsection{The GFP profile}
\label{S:method.gfpprofile}

Images were grouped according to frame and channel. Images of the same frame and the same channel could be organised into a 3D block of pixels with X, Y and Z coordinates. 3D smoothing and noise-reduction could then be applied to this structure. Each column of pixels, or a single line of pixels with a fixed XY coordinate and the full range of Z coordinates, could be used to build up an intensity distribution in Z. This is known in this study as a ``profile". The GFP channel provides 3D data since levels can be optically separated from each other [[[ref]]]. High intensity at a certain Z level indicates the presence of an object at that location in the original environment. Typically, the profile will have a low value when passing through the background, and a high value when passing through an object. This leads to a peak near the centre of an object. The location of the peak in Z and other properties of the distribution can allow pixels from the brightfield that show clear edges to be selected and compiled into a useful image.

Figure [[[fig]]] shows an example of a profile for a single pixel. The profile can be generated using two parameters; namely, the amount of gaussian smoothing applied to the image to reduce noise, [[[sigma]]], and the radius of the column in XY around the position of the profile, [[[r]]]. Both of these parameters can be adjusted to affect the output. A discussion of their optimum ranges in given in Section [[[section]]]. They both define forms of smoothing. [[[sigma]]] is the radius of the 3D gaussian kernel. [[[r]]] is the radius of a linear smoothing filter in XY. The distribution is then generated using the algorithm shown in Equation [[[eq]]]. The purpose of such a profile is not to locate objects. The assumption is that high resolution GFP indicates the presence of marked cell tissue. The value [[[sigma]]] was chosen to lie between 3-5 pixels to reduce noise while not completely breaking down fine image features approximately 2-3 pixels in size. A value of between 2-3 pixels was chosen for [[[r]]] to ensure that neighbouring pixels had a smooth transition between their Z levels. This would make continuous objects such as cells that are likely to lie around a single Z level easier to recognise as a whole.

Once the profile has been generated, several properties can be used to aide segmentation. Firstly, the Z position of the maximum value in the distribution represents the centre of the object containing GFP, at least at the location of that particular column. Secondly, the variance of the distribution can show how bright the GFP becomes with respect to the surrounding background. This is often more useful than absolute intensity since its value is based on the presence of a relatively large peak. This can better highlight parts of the cell whose brightness might be lower than average, but still contain an intensity peak. If the algorithm is applied to each XY location in the image, new images can be created using the values of interest such as the Z location and the variance. The image generated using the value of the Z location for each pixel is known here as [[[zMod]]]. It is a height map across the 3D environment. It does not contain enough information for segmentation, but it can be combined with the brightfield data to yield a new image, [[[zBF]]]. The brightfield does not contain intrinsic 3D data, but the sharpness of features in the image is best around a single Z location. This is the location of the object, and consequently, the location specified by [[[zMod]]]. Hence, each value in [[[zMod]]] can be used to select a brightfield value from the 3D stack of brightfield images and place it into the corresponding location in [[[zBF]]]. The result is a single 2D brightfield image containing features from all parts of the environment that are sharp and in-focus, regardless of their original Z location.

This can be further improved by finding the [[[zUnique]]] image. This is created by finding each unique Z value in [[[zMod]]] and substituting the maximum intensity in [[[zVar]]] for every point with the same Z. This results in high values within cells, regardless of original GFP intensity or profile strength.

\section{Results}
\label{S:results}

\begin{figure}
\centering\includegraphics[width=1.0\linewidth]{../img/fig4/v1/fig4_v1_annotated}
\label{fig:problems}
\caption{Two problems that can occur during the imaging process are movements of the microscope and drops in illumination. A and B show the GFP and corrected brightfield from a frame with good illumination. The cells (a and b) are clearly visible in the GFP. C and D show a frame with poor illumination. This leads to an image cells that have not been corrected, or the focal plane has been mis-estimated. Image E shows an example of artefacts in the final correct image that originate from uneven illumination due to camera movements or other fluctuations during imaging. The features outlined in red could be interpreted as edges of objects during the recognition process.}
\end{figure}

\subsection{Software implementation of the [[[zMod]]] method}

The method was implemented using a Python framework to organise the image database and perform calculations. Packages used include Numpy, Scipy, and Scikit. Numpy provides rapid array and linear algebra calculations implemented in C. Scipy provides more advanced image processing calculations such as smoothing, lighting correction, and edge detection. Scikit provides similar functionality. Once the images had been extracted, the [[[zMod]]] series was produced and piped to CellProfiler for segmentation. Tracking was achieved manually. No currently available tracking method was able to accurately track the objects in the available data. This is partly due to the quality of the data and the long time delay between frames. The database system helped to maintain the associations between images and objects in them during the manual tracking and the segmentation with CellProfiler. This allowed information from the segmentation such as edges in different channels to provide more accurate data on cell movements and shapes. For example, the [[[zUnique]]] image in Figure~\ref{fig:method_flowchart} was segmented to find edges that could form a boundary and modify the [[[zBF]]] image to produce [[[zEdge]]], yielding fewer errors as the segmentation was prevented from spilling out into the similarly coloured background.

\subsection{[[[zMod]]] method improves segmentation after focus correction}

In the original data set, there is no way to determine the correct level in the environment to use for segmentation. Cells of interest are found at all levels in the environment. No single level can be picked to correctly segment all cells. Following the modification of the brightfield stack using the [[[zMod]]] method, all visible cells in the environment are revealed and in-focus. An example of this is shown in Figure~\ref{fig:zbf}. Note the sharp edges of marked objects independent of original focus. This allows the segmentation algorithm to trace the edges of the objects more consistently by creating a clearly division between the edge of the cell and the background. Also note the finer details of the cell shape such as the protrusions. These cannot be clearly outlined when the cell is out of focus. Note also that cells that were not marked with GFP have not been corrected since no 3D data is available that describes them.

The image corrections can be compared with previous work by comparing the results of the segmentation. A ``ground truth", or accepted common example for comparison, was prepared by manually segmenting a selection of cells. Other work [[[ref Selinummi]]] uses a known reliable segmentation of basic image processing such as the maximum Z projection of the GFP. This was not used due to its poor represetation of the full extent of the cell. The quality of the data is too low to extract accurate cell shapes from the GFP. Figure~\ref{fig:segmentation} shows some more examples of segmentation of the whole environment along with a time series comparison of the data with previous methods. The [[[zEdge]]] segmentation consistently scores highly on the measurements in CDEF. This value measures the difference between the Ground Truth measurement and the area of segmentation compared with each other method. A high value signifies closely matching segmentation.

\section{Discussion}

\subsection{Software implementation and accesibility}

In its current form, the software is a single package that can be controlled through a series of command-line scripts. Each governs a step in the process. These steps are separated so that errors can be corrected or adjustment can be made to the parameters without re-running the entire process. Ideally, this would be collected into a simple ImageJ plugin that would perform the image manipulation alone. The current software was made to integrate ImageJ, complex image manipulation, an image database, and CellProfiler into a single process. First, image archives from a Leica Microscope in the form of ``.lif'' files were extracted using the bioformats ImageJ plugin. The images were then organised into a database through which object and channel associations could be made. Image processing was done through Django, a Python web framework. Django handles database connections and a web interface while providing a platform for image processing to be done, but provides no image specific functionality. Finally, system calls to CellProfiler could be composed from database associations for image sets. In this way, runs could be separated from each other by assigning a unique id to the interaction. This id would be persistent through the process to select the correct images. This also resulted in many different strains of output data resulting in small changes to the input parameters. They could then be compared, ordered, and analysed as a group.

\subsection{Limitations}

Despite the success of the focus correction, there are several situations that limit the effectiveness of the algorithm. Some are intrinsic to the operation of the microscope or limitations on physical components. Firstly, the imaging takes place over between ten and fifteen hours. During this time, the microscope hardware can move or be displaced by small movements around the setup. This directly affects the images and can disrupt the focus level or illumination. Uneven illumination can lead to brightness discontinuities or false edges after focus correction. In other words, pixels taken from the bottom of the environment could be brighter than those taken from the top and placed into the final [[[zEdge]]] image. Secondly, some assumptions made about the GFP data can lead to false corrections of certain parts of a cell. In the extremities of the cell, the levels of GFP might be lower than the centre, causing a level value for that part to incorrectly chosen from the background. This can lead to the final image of the cell containing a focussed centre and blurred extremities. Figure~\ref{fig:problems} shows examples of these problems. These problems are difficult to solve without making more assumptions about the origin of the data, but they can be detected as problems and flagged as part of the processing.

\section{Conclusion}

The [[[zMod]]] method accurately reconstructs focussed cells from general brightfield data. This can work even in environments with poor image quality. The method is fast and can potentially be adapted to be used in real-time in microscope software. It is applicable to any 3D sample that combines 3D and 2D image data. The [[[zMod]]] method provides an effective way to access cell details within large image datasets.

\section{Acknowledgements}

I would like to thank Shery Huang and Cristina Bertulli for their support and image data. Also, Xiaohao Cai and Carolla Schoenlieb for their advice and guidance surrounding image analysis and processing. This work was supported by the University of Cambridge.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

\bibliographystyle{model1-num-names}
% \bibliography{sample.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
