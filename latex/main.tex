%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,11pt,5p,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\pagenumbering{gobble}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Journal Name}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Image Paper}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Nicholas Piano}

\address{Cambridge University}

\begin{abstract}

%% Text of abstract
Accurate cell segmentation is crucial to biological and medical studies in providing representative data of cell movements and shapes to judge the effects of drugs and other environmental changes. Current methods rely on outlining the cell projected in 2D or a volumetric approximation based on 3D data. A common imaging method is confocal microscopy, where the images stored capture a range of focal planes in an environment. This provides 3D data and the ability to move an object of interest in and out of focus after the images have been captured, potentially offering accurate segmentation. However, conventional segmentation software fails to fully exploit the 3D nature of the data and is unable to make complex associations between image channels.

Here we describe a method for selecting regions of the brightfield channel that contain clear edges suitable for segmentation by determining the XYZ coordinates of bright regions in the GFP. This exploits the relationship between the coordinate systems of the image sets, since the majority of the 3D set of brightfield images is blurred and unsuitable for segmentation. Images of the environment can thus be simplified into a 2D projection that can be readily processed by CellProfiler or ImageJ. This method has been tested on breast cancer cells in a PDMS micro-fluidics environment and yields clear segmentation. This is an improvement over previous 2D methods of image pre-processing, such as maximum or mean Z-projection, which are unable to extract usable information from the 3D environment.

\end{abstract}

\begin{keyword}
Science \sep Publication \sep Complicated
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text
\section{Introduction}
\label{S:1}

Accurate measurement of cell morphodynamics is an essential part of live-cell studies. Cell shape can be measured by distinguishing dark edges and bright cell interiors and differentiating them from the background of the image in a process called segmentation. Newly developed microfluidics techniques allow in vitro studies of complex 3D environments. In order to study live cells moving in this environment, 3D imaging is done using a confocal microscope. Cell segmentation is then done using image recognition software such as CellProfiler or ImageJ. Ideally, the cell shape should be captured and analysed in 3D, but imaging limitations can prevent an adequate 3D reconstruction. These limitations include cell overheating from excessive contact with the laser used to produce a fluorescent response, lower resolution from a larger field of view that allows more cells to be imaged, and the necessary time delay between frames to allow the entire field to be imaged. These factors contribute to lower image quality, which prevents the extraction of high-quality shape data from the images. A possible solution is to use the relatively high-quality brightfield images to determine the shape of the cell.

In their 2009 paper, Selinummi et al. attempt to solve this problem by scanning the brightfield in 3D. Although brightfield data does not contain 3D information, intensity at a location will vary as an object moves into and out of focus by moving the current Z plane. This variation can indicate the presence of an object, and can be quantified mathematically to produce a highlighted image showing areas likely to contain objects. This works to a degree in a simple environment, but fails in more complex scenarios. It is unable to determine the Z location of individual objects, differentiate between objects such as different cell types and PMDS (a polymer used as a structural material), or accurately represent the boundary of a cell due to distortion.

In this study, we used 3D information from a Green Fluorescent Protein (GFP) channel to allow the segmentation of brightfield images by finding levels with clear edges. This exploits the relationship between the coordinate systems of the image sets. Conventional cell segmentation software such as CellProfiler fails when presented with a 3D set of brightfield data. The method presented in this study pre-processes the 3D image data to allow segmentation via conventional means. Since the coordinate systems of the brightfield and GFP images represent the same space, high intensity GFP at a single level, Z, and at a fixed location, XY, can indicate the presence of clear, dark edges of an object in the brightfield. For every point, XY, in the image, the corresponding Z level of high intensity GFP can be determined. This can then be used to combine all the high-quality edges into a single 2D image, which is then ready for conventional 2D segmentation.

\section{Materials and methods}
\label{S:method}

\subsection{Microscope apparatus}
Give all details of the microscope setup including makes of equipment.

1. Make of microscope, serial number
2. Pinhole size, aperture, other camera properties
3. Type of cancer cells, size, speed, etc.
4. Contrast, histograms, laser intensity
5. GFP type, staining method, timescales.
6. Timing of job imaging

\subsection{The GFP profile}
\label{S:method.gfpprofile}

Images were grouped according to frame and channel. Images of the same frame and the same channel could be organised into a 3D block of pixels with X, Y and Z coordinates. 3D smoothing and noise-reduction could then be applied to this structure. Each column of pixels, or a single line of pixels with a fixed XY coordinate and the full range of Z coordinates, could be used to build up an intensity distribution in Z. This is known in this study as a ``profile". The GFP channel provides 3D data since levels can be optically separated from each other [[[ref]]]. High intensity at a certain Z level indicates the presence of an object at that location in the original environment. Typically, the profile will have a low value when passing through the background, and a high value when passing through an object. This leads to a peak near the centre of an object. The location of the peak in Z and other properties of the distribution can allow pixels from the brightfield that show clear edges to be selected and compiled into a useful image.

Figure [[[fig]]] shows an example of a profile for a single pixel. The profile can be generated using two parameters; namely, the amount of gaussian smoothing applied to the image to reduce noise, [[[sigma]]], and the radius of the column in XY around the position of the profile, [[[r]]]. Both of these parameters can be adjusted to affect the output. A discussion of their optimum ranges in given in Section [[[section]]]. They both define forms of smoothing. [[[sigma]]] is the radius of the 3D gaussian kernel. [[[r]]] is the radius of a linear smoothing filter in XY. The distribution is then generated using the algorithm shown in Equation [[[eq]]]. The purpose of such a profile is not to locate objects. The assumption is that high resolution GFP indicates the presence of marked cell tissue. The value [[[sigma]]] was chosen to lie between 3-5 pixels to reduce noise while not completely breaking down fine image features approximately 2-3 pixels in size. A value of between 2-3 pixels was chosen for [[[r]]] to ensure that neighbouring pixels had a smooth transition between their Z levels. This would make continuous objects such as cells that are likely to lie around a single Z level easier to recognise as a whole.

Once the profile has been generated, several properties can be used to aide segmentation. Firstly, the Z position of the maximum value in the distribution represents the centre of the object containing GFP, at least at the location of that particular column. Secondly, the variance of the distribution can show how bright the GFP becomes with respect to the surrounding background. This is often more useful than absolute intensity since its value is based on the presence of a relatively large peak. This can better highlight parts of the cell whose brightness might be lower than average, but still contain an intensity peak. If the algorithm is applied to each XY location in the image, new images can be created using the values of interest such as the Z location and the variance. The image generated using the value of the Z location for each pixel is known here as [[[zMod]]]. It is a height map across the 3D environment. It does not contain enough information for segmentation, but it can be combined with the brightfield data to yield a new image, [[[zBF]]]. The brightfield does not contain intrinsic 3D data, but the sharpness of features in the image is best around a single Z location. This is the location of the object, and consequently, the location specified by [[[zMod]]]. Hence, each value in [[[zMod]]] can be used to select a brightfield value from the 3D stack of brightfield images and place it into the corresponding location in [[[zBF]]]. The result is a single 2D brightfield image containing features from all parts of the environment that are sharp and in-focus, regardless of their original Z location.

This can be further improved by finding the [[[zUnique]]] image. This is created by finding each unique Z value in [[[zMod]]] and substituting the maximum intensity in [[[zVar]]] for every point with the same Z. This results in high values within cells, regardless of original GFP intensity or profile strength.

\begin{figure}[h]
\centering\includegraphics[width=1.0\linewidth]{../img/fig1/v2/fig1_v2_annotated_names}
\label{fig:method_flowchart}
\caption{A graphical layout of the method used. At the top, the process starts with the original data for each frame; the brightfield image stack and the corresponding GFP channel. The [[[zMod]]] image and the [[[zVar]]] images are generated from the GFP alone by observing the GFP profile in Z. As outlined in Section~\ref{S:2.gfpprofile}, the [[[zMod]]] image gives the Z location of the maximum value in the GFP profile for each pixel. High value pixels represent higher Z values. The [[[zVar]]] image is generated by extracting the variance of each profile and assigning that value to each pixel. Both of these images can be modified using the [[[R]]] and [[[Sigma]]] parameters. The [[[zBF]]] image is the most important result of this study, combining the 3D information found using the GFP in [[[zMod]]] and the full range of the brightfield image stack, it places pixel values from Z levels where objects are in focus, allowing them to be accurately segmented. This data can be taken further using the [[[zUnique]]] and [[[zEdge]]] images. The [[[zUnique]]] image uses the maximum value for each unique value of Z in [[[zMod]]] and substitutes this value for the whole image. This yields high values within all parts of cells found at the same level, regardless of intensity or profile strength. The [[[zEdge]]] image superimposes the edge of [[[zUnique]]] for more confined segmentation.}
\end{figure}

\section{Results}
\label{S:results}

\subsection{Software implementation of the [[[zMod]]] method}

The method was implemented using a Python framework to organise the image database and perform calculations. Packages used include Numpy, Scipy, and Scikit. Numpy provides rapid array and linear algebra calculations implemented in C. Scipy provides more advanced image processing calculations such as smoothing, lighting correction, and edge detection. Scikit provides similar functionality. Once the images had been extracted, the [[[zMod]]] series was produced and piped to CellProfiler for segmentation. Tracking was achieved manually. No currently available tracking method was able to accurately track the objects in the available data. This is partly due to the quality of the data and the long time delay between frames. The database system helped to maintain the associations between images and objects in them during the manual tracking and the segmentation with CellProfiler. This allowed information from the segmentation such as edges in different channels to provide more accurate data on cell movements and shapes. For example, the [[[zUnique]]] image in Figure~\ref{fig:method_flowchart} was segmented to find edges that could form a boundary and modify the [[[zBF]]] image to produce [[[zEdge]]], yielding fewer errors as the segmentation spilled out into the background.

\subsection{[[[zMod]]] method improves segmentation after focus correction}

In the original data set, there is no way to determine the correct level in the environment to use for segmentation. Cells of interest are found at all levels in the environment. No single level can be picked to correctly segment all cells. 

\section{Discussion}

\subsection{Software implementation and accesibility}
\subsection{Limitations}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

\bibliographystyle{model1-num-names}
% \bibliography{sample.bib}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
